- 电商推荐特点
  - 很多推荐都是算法和人工运营共同完成的
- 京东排序模型
  - 内容过滤：敏感内容，负反馈（差评），已购（看过的），库存
  - 深度学习排序
  - 小流量地方 GBDT
- TensorFlow serving
- 多样性问题解决
  - 从第一个商品开始选，当选第二个商品的时候，会重新计算下候选集中每个商品的 score，然后选择一个 score 最高的。我们的方法是看 novelty score 候选商品的产品词分布和之前 N 个商品的产品词分布的 KL 距离
  - 选一个和已有商品最不像的商品，来更好的保证商品推荐结果的多样性



- embedding问题
  - 基于用户的协同过滤
  - 基于物品的协同过滤
  - 用户和物品向量化表示
  - 通过embedding目的 
    - 把原本比较稀疏的向量转换成稠密向量
  - embedding之后可以使用KNN 进行相似度判断

- faiss 稠密向量的相似度快速计算
  - 先聚类
  - 再建立二级索引加快相似计算

- ABtest
  - 分层实验，需要注意，流量正交
    - 每一层，应该走到不同的桶中，避免多个实验的互相干扰
    - 不同的 Layer 使用不同的哈希函数，保证每个 Layer 之间流量是正交的

#### 途家案例

- 民宿行业的数据，具有用户消费频次低，用户兴趣点不好描述等特点，基于内容和普通协同过滤的方法效果都不明显
- embedding
  - skipToGram
  - 用户一次搜索半小时之内点击的房子构建上下文
    - 认为一次搜索，用户点击过的物品具有内在联系
  - 用户搜索时跳过的房子，同一个地点没有搜到的房子 做为负样本，点击的房子作为正样本，下单的房子，作为每一条训练数据的正样本，权重提高

#### 51信用卡

- 这一类应用的特点
  - item很少
  - 用户不活跃，用户行为数据比较少，在这样的前提下如何做用户画像，进而进行用户分群
- 用户画像
  - 利用导入信用卡账单
  - 用户手机装的APP 根据APP信息
    - 预测用户性别 年龄
    - 预测用户理财属性，旅游属性
- Push优化五个关键问题: 合适的时间、以合适的文案、给合适的人、推送合适的内容、疲劳度控制
  - 合适时间怎么选择，可以通过画像
    - 看平均活跃时间
    - 最活跃时间
    - 最活跃时间提前1小时
    - 以及活跃次数90分位数的最早时间等，这样就可以划分活动分布
  - 文案，同一个产品，有多个文案
    - MAB算法来选择最佳文案
  - 如何通过数据驱动设计疲劳度控制？

#### GBDT （Gradient Boost Decision Tree 梯度提升决策树）

- GBDT是以决策树为基学习器的迭代算法，注意GBDT里的决策树都是回归树而不是分类树。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。 
- GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。 
  GBDT优点是适用面广，离散或连续的数据都可以处理，几乎可用于所有回归问题（线性/非线性），亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。缺点是由于弱分类器的串行依赖，导致难以并行训练数据。

三，随机森林（RF）和GBDT的区别：
随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。
组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。
组成随机森林的树可以并行生成；而GBDT只能是串行生成。
对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。
随机森林对异常值不敏感；GBDT对异常值非常敏感。
随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。
随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。



**Bagging:**

先介绍Bagging方法：

Bagging即套袋法，其算法过程如下：

1. 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
2. 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
3. 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

 

**Boosting：**

​      AdaBoosting方式每次使用的是全部的样本，每轮训练改变样本的权重。下一轮训练的目标是找到一个函数f 来拟合上一轮的残差。当残差足够小或者达到设置的最大迭代次数则停止。Boosting会减小在上一轮训练正确的样本的权重，增大错误样本的权重。（对的残差小，错的残差大）

​      梯度提升的Boosting方式是使用代价函数对上一轮训练出的模型函数f的偏导来拟合残差。

#### Bagging和Boosting的区别：

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

GBDT

https://www.cnblogs.com/onemorepoint/p/9790772.html

XGBoost

https://www.jianshu.com/p/352a03ace098